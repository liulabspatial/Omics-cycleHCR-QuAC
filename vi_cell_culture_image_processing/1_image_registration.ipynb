{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b69ae1-ff13-43a9-bd0a-18d9a072d894",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tifffile \n",
    "import numpy as np\n",
    "from bigstream.align import alignment_pipeline\n",
    "from bigstream.transform import apply_transform\n",
    "import napari\n",
    "\n",
    "import nd2 # reading nd2 file\n",
    "import dask # allows reading data in blocks, used for reading large nd2 files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82176f27-0da6-4dea-a0a0-4a6097a60a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "spacing = np.array([1, .17, .17])\n",
    "C = [0,1,2,3] \n",
    "# ZCYX.tiff\n",
    "\n",
    "affine_kwargs = {\n",
    "    'alignment_spacing':2.0,\n",
    "    'shrink_factors':(2,),\n",
    "    'smooth_sigmas':(2.,),\n",
    "    'optimizer_args':{\n",
    "        'learningRate':0.25,\n",
    "        'minStep':0.,\n",
    "        'numberOfIterations':800, \n",
    "    },\n",
    "}\n",
    "\n",
    "steps = [('affine', affine_kwargs,),] # an option here is to deform: ('deform', deform_kwargs,) see deformable_align function in align.py and configure_irm.py for parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce80b1f5-bc30-46e3-ab99-afe98e8d33ca",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for n in range(0,42):     # loop for all fields of view\n",
    "    print(n)\n",
    "    fix_path = '.\\images\\Time00002_Channel640,561,488,405_Seq0002.nd2'\n",
    "    fix = nd2.imread(fix_path,dask=True)[n,:,3,:,:].compute() # read DAPI channel, the 4th channel\n",
    "    \n",
    "    for x in range(0,7):    # loop for all cycles, each cycle as a 4-channel image\n",
    "        print(x)\n",
    "        mov_path = '.\\images\\Time0000'+ str(x) + '_Channel640,561,488,405_Seq000' + str(x) + '.nd2'\n",
    "        mov = nd2.imread(mov_path,dask=True)[n,:,3,:,:].compute()\n",
    "        print(fix.shape,mov.shape)\n",
    "        \n",
    "        affine = alignment_pipeline(\n",
    "        fix, mov, # fix data first\n",
    "        spacing, spacing, # fix spacing first, but the spacing is the same in this case\n",
    "        steps=steps\n",
    "    )\n",
    "\n",
    "    # np.savetxt('Affine'+str(x)+'.mat', affine)\n",
    "    # # load result\n",
    "    # affine = np.loadtxt('Affine'+str(x)+'.mat')\n",
    "\n",
    "        C = [0,1,2,3]\n",
    "        for c in C:     # apply transformation for all channels\n",
    "            print(c)\n",
    "            # mov2 = tifffile.imread(mov_path)[:,c,:,:].squeeze()      # for tiff files\n",
    "            mov2 = nd2.imread(mov_path,dask=True)[n,:,c,:,:].compute()      # for nd2 files\n",
    "            \n",
    "            aligned = apply_transform(\n",
    "                fix, mov2, spacing, spacing,\n",
    "                transform_list=[affine,], # 'affine' input here is the transformation matrix we got based on DAPI data\n",
    "            )\n",
    "            \n",
    "            out_path = './Bigstream_notebook/Aligned_zstack'+ str(n) + '_Time'+str(x)+ '_channel'+str(c)+'.tiff'\n",
    "            print(out_path)\n",
    "            tifffile.imwrite(out_path, aligned, imagej=True, metadata={'axes':'ZYX'})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2150b871-b490-4a7d-9005-94ea7557a682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use napari to look at the two DAPI data\n",
    "# use subsampling of 8,4,4, the images have lots of data, but we don't actually need to see all of the data points\n",
    "\n",
    "fix_img = fix[::8,::4,::4]\n",
    "mov_img = mov[::8,::4,::4]\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(fix_img,colormap='reds',opacity=0.4, blending='additive')\n",
    "viewer.add_image(mov_img,colormap='blues',opacity=0.4, blending='additive')\n",
    "\n",
    "# swtich between the channels, or use different colors and decrease opacity to check if they are exactly aligned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11f0025c-f3db-4b0b-b25e-2fbd9fd68597",
   "metadata": {},
   "outputs": [],
   "source": [
    "### we can also use napari to compare 3 data: fix, transformed mov, untransformed mov\n",
    "aligned_img = aligned[::8,::4,::4]\n",
    "\n",
    "viewer = napari.Viewer()\n",
    "viewer.add_image(fix_img,colormap='reds', opacity=0.4, blending='additive')\n",
    "viewer.add_image(mov_img,colormap='blues', opacity=0.4, blending='additive')\n",
    "viewer.add_image(aligned_img,colormap='greens', opacity=0.4, blending='additive')\n",
    "\n",
    "# aligned data should look more like the fix data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db50a143-5e6c-40af-ac24-684fed3aa9ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
