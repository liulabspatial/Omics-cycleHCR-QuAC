{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c528915b-80eb-4db3-8df6-7a24b4142221",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pprint\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "import torch\n",
    "torch.cuda.is_available()\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "from monai.apps import download_and_extract\n",
    "from monai.config import print_config\n",
    "from monai.data import Dataset, CacheDataset, DataLoader, ThreadDataLoader\n",
    "from monai.metrics import ROCAUCMetric\n",
    "from monai.networks.nets import DenseNet169\n",
    "from monai.transforms import (\n",
    "    Activations,\n",
    "    AsDiscrete,\n",
    "    EnsureChannelFirstd,\n",
    "    EnsureTyped,\n",
    "    Compose,\n",
    "    ConcatItemsd,\n",
    "    LoadImaged,\n",
    "    RandFlipd,\n",
    "    RandRotate90d,\n",
    "    RandZoomd,\n",
    "    ScaleIntensityRangePercentilesd,\n",
    ")\n",
    "from monai.utils import set_determinism\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "print_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af0312fe-1d06-4039-b00a-eed1aa53499c",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "870ad4ef-6d52-4732-a104-be53f8967efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify dataset path holding training and validation datasets of a nuclear protein\n",
    "base_path = \"./QUAC_cortex_types/H3K4me1/\"            # ./QUAC_cortex_types/mH2A1/\n",
    "train_dir = os.path.join(base_path, \"train\")\n",
    "val_dir = os.path.join(base_path, \"val\")\n",
    "\n",
    "# List all class names (sub-directory names)\n",
    "classes = sorted(os.listdir(train_dir))  # ['astrocyte', 'neuron']\n",
    "class_map = {c: idx for idx, c in enumerate(classes)}\n",
    "num_classes = len(classes)\n",
    "\n",
    "# Encode labels using class_map\n",
    "def encode_label(path):\n",
    "    class_name = os.path.basename(os.path.dirname(path))\n",
    "    return class_map[class_name]\n",
    "\n",
    "# Build a list of image paths and their corresponding labels\n",
    "def build_datalist(data_dir):\n",
    "    datalist = []\n",
    "    for class_name in os.listdir(data_dir):\n",
    "        class_dir = os.path.join(data_dir, class_name)\n",
    "        for image_name in os.listdir(class_dir):\n",
    "            image_path = os.path.join(class_dir, image_name)\n",
    "            label = encode_label(image_path)\n",
    "            datalist.append({\"image\": image_path, \"label\": label})\n",
    "    return datalist\n",
    "\n",
    "train_datalist = build_datalist(train_dir)\n",
    "val_datalist = build_datalist(val_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a87295-61e4-4f02-ada9-79b31d475489",
   "metadata": {},
   "outputs": [],
   "source": [
    "pp = pprint.PrettyPrinter()\n",
    "pp.pprint(train_datalist[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99922558-3651-4e08-8005-61d7e0fdc532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your folder-to-class mapping at the top\n",
    "folder_to_class = {\n",
    "    \"0\": \"CTX-Glut\",\n",
    "    \"5\": \"CTX-Olig\",\n",
    "    \"8\": \"CTX-GABA\",\n",
    "    \"11\": \"CTX-Astro\"\n",
    "}\n",
    "\n",
    "# Create the class_map that maps folder names to sequential indices\n",
    "classes = sorted(os.listdir(train_dir))  # ['0', '11', '5', '8']\n",
    "class_map = {c: idx for idx, c in enumerate(classes)}  # '0'->0, '11'->1, '5'->2, '8'->3\n",
    "\n",
    "# Create inverse mapping for visualization: sequential index -> class name\n",
    "class_map_inv = {idx: folder_to_class[folder_name] for folder_name, idx in class_map.items()}\n",
    "# This gives: {0: 'CTX-Glut', 1: 'CTX-Astro', 2: 'CTX-Olig', 3: 'CTX-GABA'}\n",
    "\n",
    "num_classes = len(classes)\n",
    "class_names = list(class_map_inv.values())\n",
    "\n",
    "# Rest of your code stays the same..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50badf56-c66f-4c54-849e-5dc3de07bff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(class_map)\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cc2b5b3-48f2-474e-96bd-c09d0ce57edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize test batch with normalization\n",
    "set_determinism(seed=0)\n",
    "transforms_visualize = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=['image'], reader=\"PILReader\", image_only=True),\n",
    "        EnsureChannelFirstd(keys=['image']),\n",
    "        ScaleIntensityRangePercentilesd(\n",
    "            keys=['image'], lower=1.0, upper=99.0, b_min=0.0, b_max=1.0, clip=True\n",
    "        ),\n",
    "        # ConcatItemsd(keys=[\"image\"], name=\"image\", dim=0),\n",
    "    ]\n",
    ")\n",
    "\n",
    "batch_size_viz = 6\n",
    "viz_ds = Dataset(train_datalist, transform=transforms_visualize)\n",
    "viz_loader = DataLoader(viz_ds, batch_size=batch_size_viz, shuffle=True, num_workers=0)\n",
    "batch_data = next(iter(viz_loader))\n",
    "\n",
    "fig, axs = plt.subplots(1, batch_size_viz, figsize=(15,5))   #(10, 10 * batch_size_viz), dpi=100)\n",
    "for idx in range(batch_size_viz):\n",
    "    img = batch_data['image'][idx].squeeze().numpy()  # Remove channel dimension if needed\n",
    "    axs[idx].imshow(img, cmap=\"gray\")\n",
    "    axs[idx].axis(\"off\")\n",
    "    label = int(batch_data['label'][idx])\n",
    "    label_name = class_map_inv.get(label, f\"Unknown ({label})\")\n",
    "    axs[idx].set_title(label_name)\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fdbce51-47a1-4dcb-a124-bee365d3083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 4 # Replace with your actual number of classes.\n",
    "\n",
    "# Keys for your dataset\n",
    "keys = [\"image\"]\n",
    "\n",
    "# Training Transforms\n",
    "transforms_train = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=keys, reader=\"PILReader\", image_only=True),\n",
    "        EnsureChannelFirstd(keys=keys),\n",
    "        ScaleIntensityRangePercentilesd(\n",
    "            keys=keys, lower=1.0, upper=99.0, b_min=0.0, b_max=1.0, clip=True\n",
    "        ),\n",
    "        ConcatItemsd(keys=keys, name=\"image\", dim=0),\n",
    "        EnsureTyped(keys=[\"image\", \"label\"], track_meta=False),\n",
    "        RandRotate90d(keys=[\"image\"], prob=0.75),\n",
    "        RandFlipd(keys=[\"image\"], spatial_axis=[0, 1], prob=0.5),\n",
    "        RandZoomd(keys=[\"image\"], min_zoom=0.9, max_zoom=1.1, prob=0.5),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Validation Transforms\n",
    "transforms_val = Compose(\n",
    "    [\n",
    "        LoadImaged(keys=keys, reader=\"PILReader\", image_only=True),\n",
    "        EnsureChannelFirstd(keys=keys),\n",
    "        ScaleIntensityRangePercentilesd(\n",
    "            keys=keys, lower=1.0, upper=99.0, b_min=0.0, b_max=1.0, clip=True\n",
    "        ),\n",
    "        ConcatItemsd(keys=keys, name=\"image\", dim=0),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Output Transformations\n",
    "y_pred_trans = Compose([Activations(softmax=True)])\n",
    "y_trans = Compose([AsDiscrete(to_onehot=num_classes)])\n",
    "\n",
    "# Training Dataset and DataLoader\n",
    "batch_size_train = 8\n",
    "train_ds = CacheDataset(data=train_datalist, transform=transforms_train, num_workers=10)\n",
    "train_loader = ThreadDataLoader(train_ds, batch_size=batch_size_train, shuffle=True)\n",
    "\n",
    "# Validation Dataset and DataLoader\n",
    "val_ds = CacheDataset(data=val_datalist, transform=transforms_val, num_workers=10)\n",
    "val_loader = ThreadDataLoader(val_ds, batch_size=batch_size_train, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc4aa96-dd5a-466e-a30f-4ecb443a498e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load DenseNet169 model for training\n",
    "device = \"cuda:0\"\n",
    "model = DenseNet169(spatial_dims=2, in_channels=1, out_channels=num_classes, pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bee2e3f-e885-4bdf-ae34-53047a640f24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Or load a trained model for further training\n",
    "\n",
    "device = torch.device(\"cuda:0\")\n",
    "\n",
    "# Create the same model architecture as before\n",
    "model = DenseNet169(\n",
    "    spatial_dims=2,   # 2D images\n",
    "    in_channels=1,    # single-channel images\n",
    "    out_channels=4,\n",
    "    pretrained=False  # Turn off ImageNet weights when resuming your own\n",
    ")\n",
    "\n",
    "# Load your trained weights\n",
    "model.load_state_dict(torch.load(\"QUAC_model/all_types_CTX_model/H3K4me1_55epochs_model.pth\", map_location=device))\n",
    "\n",
    "model.to(device)\n",
    "model.train() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87973a2e-b257-4096-a79a-7b2a69bf833a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), 5e-5)   # training rate: 5e-5\n",
    "max_epochs = 50   # specify number of epochs to train\n",
    "val_interval = 1\n",
    "auc_metric = ROCAUCMetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb99fa07-7a6e-4923-bd67-8173c002f65b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "best_metric = -1\n",
    "best_metric_epoch = -1\n",
    "epoch_loss_values = []\n",
    "metric_values = []\n",
    "\n",
    "for epoch in range(max_epochs):\n",
    "    print(\"-\" * 10)\n",
    "    print(f\"epoch {epoch + 1}/{max_epochs}\")\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    step = 0\n",
    "    for batch_data in train_loader:\n",
    "        step += 1\n",
    "        inputs = batch_data[\"image\"].to(device)\n",
    "        labels = batch_data[\"label\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_function(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "        print(f\"{step}/{len(train_ds) // train_loader.batch_size}, \" f\"train_loss: {loss.item():.4f}\")\n",
    "        epoch_len = len(train_ds) // train_loader.batch_size\n",
    "    epoch_loss /= step\n",
    "    epoch_loss_values.append(epoch_loss)\n",
    "    print(f\"epoch {epoch + 1} average loss: {epoch_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e124ec35-f5bd-41c1-960f-a04b589eb231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate the trained classification model\n",
    "\n",
    "batch_size_test = 32\n",
    "test_ds = Dataset(val_datalist, transform=transforms_val)\n",
    "test_loader = DataLoader(test_ds, batch_size=batch_size_test, shuffle=True, num_workers=4)\n",
    "\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "with torch.no_grad():\n",
    "    for test_data in test_loader:\n",
    "        test_images, test_labels = (\n",
    "            test_data[\"image\"].to(device),\n",
    "            test_data[\"label\"].to(device),\n",
    "        )\n",
    "        pred = model(test_images).argmax(dim=1)\n",
    "        for i in range(len(pred)):\n",
    "            y_true.append(test_labels[i].item())\n",
    "            y_pred.append(pred[i].item())\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=class_names, digits=4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47017c4c-2b01-4e06-bb06-bfea1dd9b689",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save trained model\n",
    "torch.save(model.state_dict(), \"QUAC_model/all_types_CTX_model/H3K4me1_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6153f501-1101-4e71-9c10-6d2706368da9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert model to torch-script for QuAC\n",
    "model_script = torch.jit.script(model)\n",
    "model_script.save(\"QUAC_model/all_types_CTX_model/H3K4me1_jit.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd64d01e-e01f-4f4d-8acb-89beac0281b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95507e7e-c528-4c69-9728-ae4722f32262",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
